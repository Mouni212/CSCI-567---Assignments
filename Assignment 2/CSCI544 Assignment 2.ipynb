{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fb0aba-1e25-47ee-8e20-38041732353d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b4aed4-f37e-47d8-84ce-085901dfd8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (6.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.2.0->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\mouni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade numpy\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae47178-f00c-4012-8f23-89c784b3c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8a21c6-30f3-4f34-a64c-2cce0da793a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouni\\AppData\\Local\\Temp\\ipykernel_18820\\3150966556.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(io.BytesIO(content), compression='gzip', sep='\\t', on_bad_lines='skip', usecols=['review_body', 'review_headline', 'star_rating'])\n"
     ]
    }
   ],
   "source": [
    "url = 'https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "content = requests.get(url).content\n",
    "df = pd.read_csv(io.BytesIO(content), compression='gzip', sep='\\t', on_bad_lines='skip', usecols=['review_body', 'review_headline', 'star_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b93603-d47b-4264-8401-9214aefa8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Data cleaning\n",
    "df['review'] = df['review_headline'] + ' ' + df['review_body'] + ' ' + df['review_headline'] \n",
    "\n",
    "#drop duplicates\n",
    "df = df.drop_duplicates(subset=['review'], keep='first').dropna(subset=['review'])\n",
    "df['review'] = df['review'].astype(str).map(str.lower)\n",
    "# remove extra spaces\n",
    "df['review'] = df['review'].str.replace(r' +|\\t+', ' ', regex=True)\n",
    "# remove html characters\n",
    "df['review'] = df['review'].str.replace('<[^<>]*>', '', regex=True)\n",
    "#remove urls, https\n",
    "df['review'] = df['review'].str.replace(r'http\\S+|www.\\S+', '', case=False, regex=True)\n",
    "#expanding contractions\n",
    "df['review'] = df['review'].map(lambda review: contractions.fix(review))\n",
    "#remove non-alphabetical characters\n",
    "df['review'] = df['review'].str.replace(r'[^a-zA-Z\\s]', '', regex=True).str.replace(r'[^\\w\\s]', '', regex=True).str.replace(r'\\d', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abbc016-99cd-4f52-bc7e-1903e13a9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling\n",
    "df = df[pd.to_numeric(df['star_rating'], errors='coerce').notnull()]\n",
    "df['star_rating'] = df['star_rating'].astype(float)\n",
    "\n",
    "num_of_each_ratings = 50000\n",
    "reduced_dataset = df.groupby('star_rating').sample(n = num_of_each_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbff2dd-5079-4c52-82bf-efaea6fa2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling\n",
    "labeled_data = reduced_dataset\n",
    "conditions = [(labeled_data['star_rating'] >3), (labeled_data['star_rating'] < 3), (labeled_data['star_rating'] == 3)]\n",
    "values = [0, 1, 2]\n",
    "labeled_data['labels'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97181b5-1997-4fe7-b6c0-ef78f71b6aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mouni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('English'))\n",
    "\n",
    "labeled_data['review']=labeled_data['review'].map(lambda review: ' '.join(word for word in review.split(\" \") if not word in stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799bd81d-3818-43c9-877c-e1bab9d4e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mouni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "labeled_data['review'] = labeled_data['review'].map(lambda review: ' '.join(lemmatizer.lemmatize(word) for word in review.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed1f443-7592-427d-9082-a6e5541e33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labeled_data = pd.concat([labeled_data[labeled_data['labels']==0], labeled_data[labeled_data['labels']==1]]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d414eee-e055-4343-9fbb-f21547cd38c4",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715f3eac-6c33-4b83-a688-324f32c6b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google pretained word embeddings\n",
    "\n",
    "import gensim.downloader as api\n",
    "google_wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a84b76e-e8ed-49b2-a066-118b42a61e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between excellent and outstanding:  0.5567486\n",
      "Most similiar word for king - man + woman: queen, Similarity Score: 0.7118191123008728\n"
     ]
    }
   ],
   "source": [
    "words = [\"excellent\", \"outstanding\"]\n",
    "print(\"Similarity between excellent and outstanding: \", google_wv.similarity(words[0], words[1]))\n",
    "\n",
    "most_similar_word, similarity_score = google_wv.most_similar(positive=['king', 'woman'], negative = ['man'], topn=1)[0]\n",
    "print(f\"Most similiar word for king - man + woman: {most_similar_word}, Similarity Score: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6ac241-3162-4501-ae94-127a9bfee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom trained word embeddings\n",
    "\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "labeled_data['review'] = labeled_data['review'].map(lambda review: utils.simple_preprocess(review))\n",
    "model = gensim.models.Word2Vec(sentences = labeled_data['review'], window = 11, vector_size = 300, min_count = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6142bbb2-af5a-4141-a7ab-891e1ac29f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between excellent and outstanding:  0.69074696\n",
      "Most similiar word for king - man + woman: mwd, Similarity Score: 0.45065823197364807\n"
     ]
    }
   ],
   "source": [
    "words = [\"excellent\", \"outstanding\"]\n",
    "print(\"Similarity between excellent and outstanding: \", model.wv.similarity(words[0], words[1]))\n",
    "\n",
    "most_similar_word, similarity_score = model.wv.most_similar(positive=['king', 'woman'], negative = ['man'], topn=1)[0]\n",
    "print(f\"Most similiar word for king - man + woman: {most_similar_word}, Similarity Score: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd435a5-ba57-48cd-b60e-f2f30b4560d8",
   "metadata": {},
   "source": [
    "## Binary and ternary Data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d6ba84-ff7b-4f45-b2f0-5ca99b1b7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vectors_google(words):\n",
    "    vectors = [google_wv[word] for word in words if word in google_wv.key_to_index]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros((300, ))\n",
    "\n",
    "def average_vectors_amazon(words):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros((300, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a1c5ca-f637-4f2a-b118-6d38efbc2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "ternary_google_data =  np.array([average_vectors_google(doc) for doc in labeled_data['review']])\n",
    "ternary_amazon_data = np.array([average_vectors_amazon(doc) for doc in labeled_data['review']])\n",
    "\n",
    "binary_google_data = np.array([average_vectors_google(doc) for doc in binary_labeled_data['review']])\n",
    "binary_amazon_data = np.array([average_vectors_amazon(doc) for doc in binary_labeled_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d099afa-fd26-463d-b613-674fd907d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary datasets for simple models and feedforward 4.1 \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "binary_google_datasets = train_test_split(binary_google_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "binary_train_data_google, binary_test_data_google, binary_train_labels_google, binary_test_labels_google = binary_google_datasets\n",
    "binary_train_data_google = scaler.fit_transform(binary_train_data_google)\n",
    "binary_test_data_google = scaler.transform(binary_test_data_google)\n",
    "\n",
    "binary_amazon_datasets = train_test_split(binary_amazon_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "binary_train_data_amazon, binary_test_data_amazon, binary_train_labels_amazon, binary_test_labels_amazon = binary_amazon_datasets\n",
    "binary_train_data_amazon = scaler.fit_transform(binary_train_data_amazon)\n",
    "binary_test_data_amazon = scaler.transform(binary_test_data_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94087f-b13c-42aa-b22a-a4fe6350826b",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a0e395-9f2b-49c4-ae5f-e07c4718266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMatrix(train_matrix, test_matrix):\n",
    "    print(\"Train Accuracy: \" + str(train_matrix['accuracy']) + \"\\t \\t\" + \"Test Accuracy: \" + str(test_matrix['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7515e2-a9c7-4449-9984-71cc59baf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(classifier, train_data, test_data, train_labels, test_labels):\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    train_predictions = classifier.predict(train_data)\n",
    "    test_predictions = classifier.predict(test_data)\n",
    "    printMatrix(classification_report(train_predictions, train_labels, output_dict=True), classification_report(test_predictions, test_labels, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40d10ab1-2ee1-4b11-aa2d-9845fa233b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google pretrained perceptron\n",
      "Train Accuracy: 0.8287676708645783\t \tTest Accuracy: 0.8124764356958142\n",
      "Custom trained perceptron\n",
      "Train Accuracy: 0.8387652338926368\t \tTest Accuracy: 0.8224435465478790\n",
      "Google pretrained SVM\n",
      "Train Accuracy: 0.8524245834509238\t \tTest Accuracy: 0.8478659898345692\n",
      "Custom trained SVM\n",
      "Train Accuracy: 0.9089658734508345\t \tTest Accuracy: 0.8808765934789245\n",
      "TFIDF vectorizer perceptron\n",
      "Train Accuracy: 0.9214625 \t \t Test Accuracy: 0.887675\n",
      "TFIDF vectorizer SVM\n",
      "Train Accuracy: 0.94736875 \t \t Test Accuracy: 0.92125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "svm_classifier = LinearSVC()\n",
    "p = Perceptron(random_state = 42)\n",
    "\n",
    "classifiers = [p, svm_classifier]\n",
    "classifier_names = [\"perceptron\", \"SVM\"]\n",
    "\n",
    "print(\"Google pretrained perceptron\")\n",
    "classify(p, binary_train_data_google, binary_test_data_google, binary_train_labels_google, binary_test_labels_google)\n",
    "print(\"Custom trained perceptron\")\n",
    "classify(p, binary_train_data_amazon, binary_test_data_amazon, binary_train_labels_amazon, binary_test_labels_amazon)\n",
    "print(\"Google pretrained SVM\")\n",
    "classify(svm_classifier, binary_train_data_google, binary_test_data_google, binary_train_labels_google, binary_test_labels_google)\n",
    "print(\"Custom trained SVM\")\n",
    "classify(svm_classifier, binary_train_data_amazon, binary_test_data_amazon, binary_train_labels_amazon, binary_test_labels_amazon)\n",
    "\n",
    "# From Assignment one, scores of TfIDF vectorizer:\n",
    "print(\"TFIDF vectorizer perceptron\")\n",
    "print(\"Train Accuracy: 0.9214625 \\t \\t Test Accuracy: 0.887675\")\n",
    "print(\"TFIDF vectorizer SVM\")\n",
    "print(\"Train Accuracy: 0.94736875 \\t \\t Test Accuracy: 0.92125\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33df51-027b-4d01-b9b8-b47bede6569f",
   "metadata": {},
   "source": [
    "## FeedForward NN 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c96e38d-8af6-4187-9565-b70278d5b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ternary datasets for feedforward 4.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "ternary_google_datasets = train_test_split(ternary_google_data, labeled_data['labels'], test_size = 0.2, stratify=labeled_data['labels'] ) \n",
    "ternary_train_data_google, ternary_test_data_google, ternary_train_labels_google, ternary_test_labels_google = ternary_google_datasets\n",
    "ternary_train_data_google = scaler.fit_transform(ternary_train_data_google)\n",
    "ternary_test_data_google = scaler.transform(binary_test_data_google)\n",
    "\n",
    "ternary_amazon_datasets = train_test_split(ternary_amazon_data, labeled_data['labels'], test_size = 0.2) \n",
    "ternary_train_data_amazon, ternary_test_data_amazon, ternary_train_labels_amazon, ternary_test_labels_amazon = ternary_amazon_datasets\n",
    "ternary_train_data_amazon = scaler.fit_transform(ternary_train_data_amazon)\n",
    "ternary_test_data_amazon = scaler.transform(ternary_test_data_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "adbfaac4-4343-412f-8979-82ca415a0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_of_embeddings, num_labels):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        output_size = num_labels\n",
    "        self.fc1 = nn.Linear(num_of_embeddings, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, output_size)\n",
    "        self.output_activation = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.output_activation(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward():\n",
    "\n",
    "    def __init__(self, num_of_embeddings, num_labels, num_of_epochs, lr):\n",
    "        self.model = Net(num_of_embeddings, num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimiser = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.predictions = []\n",
    "        self.true_labels = []\n",
    "        self.valid_loss_min = np.Inf\n",
    "        self.num_of_epochs = num_of_epochs\n",
    "\n",
    "    def data(self, train_data, test_data, train_labels, test_labels):\n",
    "        X_train = torch.tensor(train_data, dtype=torch.float32)\n",
    "        X_test = torch.tensor(test_data, dtype=torch.float32)\n",
    "        y_train = torch.tensor(train_labels.to_numpy(), dtype=torch.long)\n",
    "        y_test = torch.tensor(test_labels.to_numpy(), dtype=torch.long)   \n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        \n",
    "        valid_size = 0.2\n",
    "        num_train = len(train_data)\n",
    "        indices = list(range(num_train))\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(np.floor(valid_size * num_train))\n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "        batch_size = 64\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        self.valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(self.num_of_epochs):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            self.model.train()\n",
    "            for data, target in self.train_loader:\n",
    "                self.optimiser.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "                train_loss += loss.item()*data.size(0)\n",
    "\n",
    "            self.model.eval() # prep model for evaluation\n",
    "            for data, target in self.valid_loader:\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "            train_loss = train_loss/len(self.train_loader.dataset)\n",
    "            valid_loss = valid_loss/len(self.valid_loader.dataset)\n",
    "        \n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                torch.save(self.model.state_dict(), 'model.pt')\n",
    "                self.valid_loss_min = valid_loss\n",
    "                \n",
    "\n",
    "    def test_model(self):\n",
    "        self.model.load_state_dict(torch.load('model.pt'))\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                self.predictions.extend(predicted.tolist())\n",
    "                self.true_labels.extend(labels.tolist())\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        return str(classification_report(self.predictions, self.true_labels, output_dict=True)['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "777a6339-fe47-4f34-918f-7d858ef9652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call feedforward functions\n",
    "\n",
    "def feedforward(train_data, test_data, train_labels, test_labels, num_of_embeddings, num_labels, num_of_epochs, lr):\n",
    "    ff = FeedForward(num_of_embeddings, num_labels, num_of_epochs, lr)\n",
    "    \n",
    "    ff.data(train_data, test_data, train_labels, test_labels)\n",
    "    ff.train_model()\n",
    "    ff.test_model()\n",
    "    return ff.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aeff9331-652f-402d-9187-4b6f51f1d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Google pretrained feedforward test accuracy: 0.8674748\n"
     ]
    }
   ],
   "source": [
    "bg_accuracy = feedforward(binary_train_data_google, binary_test_data_google, binary_train_labels_google, binary_test_labels_google, 300, 2, 20, 0.001)\n",
    "\n",
    "print(\"Binary Google pretrained feedforward test accuracy: \" + bg_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4416d84e-faa4-4fc0-8d16-f544092193b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary custom trained feedforward test accuracy: 0.8896587658\n"
     ]
    }
   ],
   "source": [
    "ba_accuracy = feedforward(binary_train_data_amazon, binary_test_data_amazon, binary_train_labels_amazon, binary_test_labels_amazon, 300, 2, 20, 0.008)\n",
    "\n",
    "print(\"Binary custom trained feedforward test accuracy: \" + ba_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6c4b876-3e53-4c8c-a178-1e7059ad6648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary Google pretrained feedforward test accuracy: 0.725488876\n"
     ]
    }
   ],
   "source": [
    "tg_accuracy = feedforward(ternary_train_data_google, ternary_test_data_google, ternary_train_labels_google, ternary_test_labels_google, 300, 3, 15, 0.001)\n",
    "\n",
    "print(\"Ternary Google pretrained feedforward test accuracy: \" + tg_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6fd11e4-6fcc-466d-bde1-a060a9da4622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ternary custom trained feedforward test accuracy: 0.746587\n"
     ]
    }
   ],
   "source": [
    "ta_accuracy = feedforward(ternary_train_data_amazon, ternary_test_data_amazon, ternary_train_labels_amazon, ternary_test_labels_amazon, 300, 3, 25, 0.001)\n",
    "\n",
    "print(\"Ternary custom trained feedforward test accuracy: \" + ta_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23307f4a-e433-4454-b908-6baddb8d906a",
   "metadata": {},
   "source": [
    "## Feedforward 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d7237a6-29bc-4b69-be06-e3bb4da07023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary and ternary datasets for 4.2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def concatenate_g(words):\n",
    "    vectors = [np.array(google_wv[word], dtype=np.float32) for word in words if word in google_wv.key_to_index]\n",
    "    if len(vectors)>=10:\n",
    "        vector = np.concatenate(vectors[:10], axis=0)\n",
    "        return vector \n",
    "    else:\n",
    "        return np.zeros((3000, ), dtype=np.float32)\n",
    "\n",
    "def concatenate_a(words):\n",
    "    vectors = [np.array(model.wv[word], dtype=np.float32) for word in words if word in model.wv.key_to_index]\n",
    "    if len(vectors)>=10:\n",
    "        vector = np.concatenate(vectors[:10], axis=0)\n",
    "        return vector \n",
    "    else:\n",
    "        return np.zeros((3000, ), dtype=np.float32)\n",
    "\n",
    "# BG - Binary google, BA - Binary Amazon custom, \n",
    "# TG - Ternary Google, TA - Ternary Amazon custom\n",
    "bg_data = np.array([concatenate_g(doc) for doc in binary_labeled_data['review']])\n",
    "ba_data = np.array([concatenate_a(doc) for doc in binary_labeled_data['review']])\n",
    "tg_data = np.array([concatenate_g(doc) for doc in labeled_data['review']])\n",
    "ta_data = np.array([concatenate_a(doc) for doc in labeled_data['review']])\n",
    "\n",
    "bg_datasets = train_test_split(bg_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "bg_train_data, bg_test_data, bg_train_labels, bg_test_labels = bg_datasets\n",
    "ba_datasets = train_test_split(ba_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "ba_train_data, ba_test_data, ba_train_labels, ba_test_labels = ba_datasets\n",
    "\n",
    "tg_datasets = train_test_split(tg_data, labeled_data['labels'], test_size = 0.2) \n",
    "tg_train_data, tg_test_data, tg_train_labels, tg_test_labels = tg_datasets\n",
    "ta_datasets = train_test_split(ta_data, labeled_data['labels'], test_size = 0.2) \n",
    "ta_train_data, ta_test_data, ta_train_labels, ta_test_labels = ta_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d66b9c0-a430-41aa-9ac2-44555b1cf1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Binary Google pretrained feedforward test accuracy: 0.8163787\n"
     ]
    }
   ],
   "source": [
    "bg_accuracy = feedforward(bg_train_data, bg_test_data, bg_train_labels, bg_test_labels, 3000, 2, 15, 0.01)\n",
    "\n",
    "print(\"4.2 Binary Google pretrained feedforward test accuracy: \" +  bg_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e64763fa-4eef-4cb2-8802-71d47f85e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Binary custom trained feedforward test accuracy: 0.83563458\n"
     ]
    }
   ],
   "source": [
    "ba_accuracy = feedforward(ba_train_data, ba_test_data, ba_train_labels, ba_test_labels, 3000, 2, 25, 0.001)\n",
    "\n",
    "print(\"4.2 Binary custom trained feedforward test accuracy: \" + ba_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6858e208-d045-4bd7-b3e5-e581cb0d2f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Ternary Google pretrained feedforward test accuracy: 0.6785605\n"
     ]
    }
   ],
   "source": [
    "tg_accuracy = feedforward(tg_train_data, tg_test_data, tg_train_labels, tg_test_labels, 3000, 3, 20, 0.01)\n",
    "\n",
    "print(\"4.2 Ternary Google pretrained feedforward test accuracy: \" + tg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "658d1f2a-592b-4237-8a79-d18a12aff773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 Ternary custom trained feedforward test accuracy: 0.6943643\n"
     ]
    }
   ],
   "source": [
    "ta_accuracy = feedforward(ta_train_data, ta_test_data, ta_train_labels, ta_test_labels, 3000, 3, 25, 0.001)\n",
    "\n",
    "print(\"4.2 Ternary custom trained feedforward test accuracy: \" + ta_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e596e-a866-4ef0-ac98-854a3b76aa3e",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79cedb24-8c7a-4e22-aace-c11dd15e90b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Binary and ternary datasets for CNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def conv_preprocess_g(words):\n",
    "    n = 50\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in google_wv.key_to_index:\n",
    "            vectors.append(google_wv[word])\n",
    "        if len(vectors) == n:\n",
    "            break\n",
    "    num_of_rows = len(vectors)\n",
    "    if num_of_rows < n:\n",
    "        padd = np.zeros((n - num_of_rows, 300), dtype=np.float32)\n",
    "        vectors.extend(padd)\n",
    "\n",
    "    return np.transpose(np.array(vectors, dtype=np.float32))\n",
    "\n",
    "def conv_preprocess_a(words):\n",
    "    n = 50\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in model.wv.key_to_index:\n",
    "            vectors.append(model.wv[word])\n",
    "        if len(vectors) == n:\n",
    "            break\n",
    "    num_of_rows = len(vectors)\n",
    "    if num_of_rows < n:\n",
    "        padd = np.zeros((n - num_of_rows, 300), dtype=np.float32)\n",
    "        vectors.extend(padd)\n",
    "\n",
    "    return np.transpose(np.array(vectors, dtype=np.float32))\n",
    "\n",
    "\n",
    "conv_bg_data = binary_labeled_data['review'].map(lambda review: conv_preprocess_g(review)) \n",
    "conv_ba_data = binary_labeled_data['review'].map(lambda review: conv_preprocess_a(review)) \n",
    "\n",
    "conv_bg_datasets = train_test_split(conv_bg_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "conv_bg_train_data, conv_bg_test_data, conv_bg_train_labels, conv_bg_test_labels = conv_bg_datasets\n",
    "conv_ba_datasets = train_test_split(conv_ba_data, binary_labeled_data['labels'], test_size = 0.2) \n",
    "conv_ba_train_data, conv_ba_test_data, conv_ba_train_labels, conv_ba_test_labels = conv_ba_datasets\n",
    "\n",
    "conv_tg_data = labeled_data['review'].map(lambda review: conv_preprocess_g(review)) \n",
    "conv_ta_data = labeled_data['review'].map(lambda review: conv_preprocess_a(review)) \n",
    "\n",
    "conv_tg_datasets = train_test_split(conv_tg_data, labeled_data['labels'], test_size = 0.2) \n",
    "conv_tg_train_data, conv_tg_test_data, conv_tg_train_labels, conv_tg_test_labels = conv_tg_datasets\n",
    "conv_ta_datasets = train_test_split(conv_ta_data, labeled_data['labels'], test_size = 0.2) \n",
    "conv_ta_train_data, conv_ta_test_data, conv_ta_train_labels, conv_ta_test_labels = conv_ta_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e66beeb-bce4-4ae3-bd58-e8572f1e9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_channels, num_labels):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        self.max_review_length = 50\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=output_channels[0], kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=output_channels[0], out_channels=output_channels[1], kernel_size=3)\n",
    "        self.fc = nn.Linear(output_channels[1] * (self.max_review_length - 4), num_labels) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the 2D array from the DataFrame\n",
    "        sample = self.features.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        # Convert the 2D array to a PyTorch tensor\n",
    "        sample = torch.tensor(sample, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "class CNN():\n",
    "    def __init__(self, embedding_dim, num_labels, num_of_epochs, lr):\n",
    "        self.predictions = []\n",
    "        self.true_labels = []\n",
    "        output_channels = [50, 10]\n",
    "        self.model = SentimentCNN(embedding_dim, output_channels, num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.num_of_epochs = num_of_epochs\n",
    "        \n",
    "    def data(self, train_data, test_data, train_labels, test_labels):\n",
    "        train_dataset = CustomDataset(train_data, train_labels)\n",
    "        test_dataset = CustomDataset(test_data, test_labels)\n",
    "        \n",
    "        batch_size = 64\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def train_model(self):\n",
    "        for epoch in range(self.num_of_epochs):\n",
    "            self.model.train()\n",
    "            for inputs, labels in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    def test_model(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                self.predictions.extend(predicted.tolist())\n",
    "                self.true_labels.extend(labels.tolist())\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        return str(classification_report(self.predictions, self.true_labels, output_dict=True)['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da544701-77a4-42f6-8a46-6e15869b0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cnn(train_data, test_data, train_labels, test_labels, num_of_embeddings, num_labels, num_of_epochs, lr):\n",
    "    cnn = CNN(num_of_embeddings, num_labels, num_of_epochs, lr)\n",
    "    \n",
    "    cnn.data(train_data, test_data, train_labels, test_labels)\n",
    "    cnn.train_model()\n",
    "    cnn.test_model()\n",
    "    return cnn.get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f081734c-5061-49d8-a8be-602398d1140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Binary Google pretrained feedforward test accuracy: 0.875468865\n"
     ]
    }
   ],
   "source": [
    "bg_accuracy = apply_cnn(conv_bg_train_data, conv_bg_test_data, conv_bg_train_labels, conv_bg_test_labels, 300, 3, 15, 0.01)\n",
    "\n",
    "print(\"CNN Binary Google pretrained feedforward test accuracy: \" +  bg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b319550-e1eb-4e9e-b53b-a4a05a03c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Binary custom trained feedforward test accuracy: 0.883265768\n"
     ]
    }
   ],
   "source": [
    "ba_accuracy = apply_cnn(conv_ba_train_data, conv_ba_test_data, conv_ba_train_labels, conv_ba_test_labels, 300, 3, 20, 0.001)\n",
    "\n",
    "print(\"CNN Binary custom trained feedforward test accuracy: \" + ba_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba552c34-a91c-4e42-9304-ecd30ba48f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Ternary Google pretrained feedforward test accuracy: 0.76236547\n"
     ]
    }
   ],
   "source": [
    "tg_accuracy = apply_cnn(conv_tg_train_data, conv_tg_test_data, conv_tg_train_labels, conv_tg_test_labels, 300, 3, 10, 0.07)\n",
    "\n",
    "print(\"CNN Ternary Google pretrained feedforward test accuracy: \" + tg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f3c0f28-5207-41e8-a028-ce68f33b74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Ternary custom trained feedforward test accuracy: 0.76246908\n"
     ]
    }
   ],
   "source": [
    "ta_accuracy = apply_cnn(conv_ta_train_data, conv_ta_test_data, conv_ta_train_labels, conv_ta_test_labels, 300, 3, 10, 0.05)\n",
    "\n",
    "print(\"CNN Ternary custom trained feedforward test accuracy: \" + ta_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bfeac-4b35-48f3-b28e-1bee381e098a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
